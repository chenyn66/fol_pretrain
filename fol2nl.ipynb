{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "import data\n",
    "import torch\n",
    "import training\n",
    "\n",
    "\n",
    "import openai\n",
    "openai.api_key = open('key.txt').read().strip()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5233 1285\n"
     ]
    }
   ],
   "source": [
    "train_dataset = data.FOL2NL(split='train')\n",
    "dev_dataset = data.FOL2NL(split='dev')\n",
    "print(len(train_dataset), len(dev_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∀x (happy(x) → isred(x)) ->\n",
      " If a cartoon character is happy, then it is red.\n"
     ]
    }
   ],
   "source": [
    "# fol = '∀x (happy(x) → ¬isred(x))'\n",
    "# fol = '∀x (happy(x) → isred(x))'\n",
    "# fol = '∃x (happy(x) ∧ isred(x))'\n",
    "# fol = '∃x (happy(x) ∧ ¬isred(x))'\n",
    "\n",
    "\n",
    "prompt = fol + ' ->'\n",
    "print(prompt)\n",
    "response = openai.Completion.create(\n",
    "    model='curie:ft-personal-2022-12-06-07-10-39',\n",
    "    prompt=prompt,\n",
    "    stop=[\"\\n\"],\n",
    "    temperature=0\n",
    "    )\n",
    "text = response['choices'][0]['text']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fol = '∀x(smile(x) → happy(x))'\n",
    "prompt = fol + ' ->'\n",
    "print(prompt)\n",
    "response = openai.Completion.create(\n",
    "    model='curie:ft-personal-2022-12-06-07-10-39',\n",
    "    prompt=prompt,\n",
    "    stop=[\"\\n\"],\n",
    "    temperature=0.7\n",
    "    )\n",
    "text = response['choices'][0]['text']\n",
    "print(text)\n",
    "print(nl)\n",
    "print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Includes(diamondmine, creedbrothers) ∧ Includes(diamondmine, ivynile) ->\n",
      " Diamond Mine includes the Creed Brothers, Ivy Nile.\n",
      "Diamond Mine includes the Creed Brothers, and Ivy Nile.\n",
      "----\n",
      "HasFeud(imperium, diamondmine) ->\n",
      " Imperium has a feud with Diamond Mine.\n",
      "Imperium has a feud with Diamond Mine.\n",
      "----\n",
      "Leads(roderickstrong, creedbrothers) ->\n",
      " Roderick strong leads the Creed Brothers.\n",
      "Roderick strong leads the Creed Brothers.\n",
      "----\n",
      "ProfessionalWrestlingStable(diamondmine) ∧ FormedIn(diamondmine, wwe) ->\n",
      " Diamond Mine is a professional wrestling stable, formed in WWE.\n",
      "Diamond Mine is a professional wrestling stable, formed in WWE.\n",
      "----\n",
      "Leads(roderickstrong, diamondmine) ->\n",
      " Roderick strong leads Diamond Mine.\n",
      "Roderick Strong leads Diamond Mine.\n",
      "----\n",
      "Includes(diamondmine, creedbrothers) ∧ Includes(diamondmine, ivynile) ->\n",
      " Diamond Mine includes the Creed Brothers, and Ivy Nile.\n",
      "Diamond Mine includes the Creed Brothers, and Ivy Nile.\n",
      "----\n",
      "HasFeud(imperium, diamondmine) ->\n",
      " Imperium has a feud with Diamond Mine.\n",
      "Imperium has a feud with Diamond Mine.\n",
      "----\n",
      "∀x (ProfessionalWrestlingStable(x) ∧ Includes(x, ivynile) → ¬HasFeud(imperium, x)) ->\n",
      " Imperium doesn't have a feud with a professional wrestling stable that includes Ivy Nile.\n",
      "Imperium doesn't have a feud with a professional wrestling stable that includes Ivy Nile.\n",
      "----\n",
      "MusicPiece(symphony9) ->\n",
      " Symphony No. 9 is a music piece.\n",
      "Symphony No. 9 is a music piece.\n",
      "----\n",
      "∀x ∀y ((MusicPiece(x) ∧ Writtenby(x, y)) → Composer(y)) ->\n",
      " Composers write music pieces.\n",
      "Composers write music pieces.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for fol, nl in dev_dataset[234:244]:\n",
    "    prompt = fol + ' ->'\n",
    "    print(prompt)\n",
    "    response = openai.Completion.create(\n",
    "        model='curie:ft-personal-2022-12-06-07-10-39',\n",
    "        prompt=prompt,\n",
    "        stop=[\"\\n\"],\n",
    "        temperature=0.7\n",
    "        )\n",
    "    text = response['choices'][0]['text']\n",
    "    print(text)\n",
    "    print(nl)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sample = set()\n",
    "with open('dataset/folio/gpt-finetune.jsonl','w',encoding='utf-8') as f:\n",
    "    for x,y in train_dataset:\n",
    "        if (x,y) in unique_sample:\n",
    "            continue\n",
    "        f.write(json.dumps({'prompt':x,'completion':y})+'\\n')\n",
    "        unique_sample.add((x,y))\n",
    "\n",
    "    for x,y in dev_dataset:\n",
    "        if (x,y) in unique_sample:\n",
    "            continue\n",
    "        f.write(json.dumps({'prompt':x,'completion':y})+'\\n')\n",
    "        unique_sample.add((x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['âĨĴ']\n",
      "['âĨ', 'Ķ']\n",
      "['Â', '¬']\n",
      "['â', 'Ĭ', 'ķ']\n",
      "['âĪ', '¨']\n",
      "['âĪ', '§']\n",
      "['âĪ', 'Ģ']\n",
      "['âĪ', 'ĥ']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "for char in train_dataset.speicial_tokens:\n",
    "    print(tokenizer.convert_ids_to_tokens(tokenizer.encode(char)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cheny\\Anaconda3\\envs\\wfsa\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32108, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "tokenizer.add_tokens(train_dataset.speicial_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=training.collate_fn(tokenizer))\n",
    "test_loder = torch.utils.data.DataLoader(dev_dataset, batch_size=8, shuffle=True, collate_fn=training.collate_fn(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.train(model, tokenizer, train_loader, test_loder, epoch=20, update_every=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('wfsa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b346918d0b7bb6a3d33c853c0503828ab6aa9b445b216f9b49c79d37806d3d3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
